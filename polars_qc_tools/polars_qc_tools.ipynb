{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aab0715-b5a6-47ef-8e60-1164a5e6028f",
   "metadata": {},
   "source": [
    "## Polars QC Tools\n",
    "\n",
    "This notebook is a reference collection of functions and tips for cleaning datasets using polars. This is not designed to be run on a specific dataset, just provide snippets for common cleaning tasks.\n",
    "\n",
    "\n",
    "### Set Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0997b7-0d8b-493e-9361-e3314f965a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e62692",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d649b9-0635-4529-9d0f-840769563311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small/medium data, can use eager mode:\n",
    "df = pl.read_csv(\"my_dataset.csv\")\n",
    "# For large data, use lazy mode:\n",
    "lf = pl.scan_csv(\n",
    "\"my_dataset.csv\",\n",
    "# separator=\",\",\n",
    "# has_header=True,\n",
    "# infer_schema_length=1000, # increase if many columns\n",
    "# ignore_errors=True, # tolerate some bad rows\n",
    "# dtypes={\"col_a\": pl.Int64, \"col_b\": pl.Utf8},\n",
    ")\n",
    "\n",
    "# optional small eager copy for quick peeks\n",
    "df = lf.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb5a5-6d8f-4535-8958-556ea2b1b69b",
   "metadata": {},
   "source": [
    "### Process large data sets (streaming)\n",
    "\n",
    "# Polars encourages a single lazy plan (like a pipeline) rather than manual\n",
    "# Python loops like I did with pandas chunks. With .scan_csv + .collect(streaming=True), Polars # streams from disk with pushdowns, similar to chunking but faster and simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360c862-1203-4205-891e-41ea99616ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example cleaner function that returns a LazyFrame expression pipeline\n",
    "def process_lazy(lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return (\n",
    "        lf.with_columns(\n",
    "            # Example vectorized cleaner on a text column\n",
    "            pl.col(\"raw_column\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.strip()\n",
    "            .str.to_lowercase()\n",
    "            .alias(\"cleaned_column\")\n",
    "        )\n",
    "        .filter(pl.col(\"cleaned_column\") == pl.lit(\"filter_value\"))\n",
    "        # keep only a subset of columns downstream\n",
    "        .select([\"cleaned_column\", pl.all().exclude(\"cleaned_column\")])\n",
    "    )\n",
    "\n",
    "\n",
    "# Build the pipeline lazily\n",
    "processed_lf = process_lazy(pl.scan_csv(\"my_large_dataset.csv\"))\n",
    "\n",
    "\n",
    "# Collect with streaming enabled (low memory)\n",
    "processed_df = processed_lf.collect(streaming=True)\n",
    "\n",
    "\n",
    "# Write to disk (CSV). When using streaming, polars will stream rows out.\n",
    "processed_df.write_csv(\"cleaned_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e94f7e-2d20-4443-9131-5cf1f69fca36",
   "metadata": {},
   "source": [
    "### Clean up columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f8de0-6c08-46bf-9a8f-77ae5d2d0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example column rename function:\n",
    "def snake_case(name: str) -> str:\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \"_\", name).strip().lower()\n",
    "    return name\n",
    "\n",
    "# Lazy example for large dataframes:\n",
    "lf_clean_cols = lf.rename({c: snake_case(c) for c in lf.collect_schema().names()})\n",
    "\n",
    "# Eager example for small dataframes:\n",
    "df = df.rename({c: snake_case(c) for c in df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22b002-abeb-40b5-b5ca-b0bd94a2cdca",
   "metadata": {},
   "source": [
    "### Fix datatypes and categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9154dfa-cc04-4389-a085-ce72f2d76f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric parsing with coercion-like behavior\n",
    "lf_types = (\n",
    "    lf_clean_cols.with_columns(\n",
    "        # Numeric downcast-like: choose explicit, then optionally cast to smaller\n",
    "        pl.col(\"num_feature\").cast(pl.Float64, strict=False),\n",
    "        # Strings (Utf8)\n",
    "        pl.col(\"str_feature\").cast(pl.Utf8, strict=False),\n",
    "        # Dates (coerce invalid to null)\n",
    "        pl.col(\"date_col\").str.strptime(pl.Date, strict=False, format=None),\n",
    "        # Clip values\n",
    "        pl.col(\"col\").clip_min(0).clip_max(100).alias(\"col\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d9190-11d4-4da0-8981-95e82d2ca05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categoricals \n",
    "lf_cats = lf_types.with_columns(\n",
    "    pl.col(\"cat_uncoded\").cast(pl.Categorical).alias(\"cat_encoded_cat\"),\n",
    "    # integer codes for categoricals\n",
    "    pl.col(\"cat_uncoded\").cast(pl.Categorical).cat.to_physical().alias(\"cat_encoded\"),\n",
    ")\n",
    "\n",
    "# One-hot/dummies\n",
    "lf_dummies = lf_cats.to_dummies(columns=[\"col\"]) # expands columns into 0/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b350080-0925-4f4b-8227-8d252ecc5623",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa260f-a8b4-4e95-a373-ca2365142d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data report:\n",
    "\n",
    "def missing_report(lf: pl.LazyFrame) -> pl.DataFrame:\n",
    "    cols = lf.collect_schema().names()\n",
    "    base = lf.select([\n",
    "        pl.len().alias(\"n_rows\"),\n",
    "        *[pl.col(c).null_count().alias(f\"{c}__nulls\") for c in cols],\n",
    "    ]).collect()\n",
    "\n",
    "\n",
    "    n = int(base[0, \"n_rows\"]) if base.height else 0\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        nulls = int(base[0, f\"{c}__nulls\"]) if n else 0\n",
    "        pct = round((nulls / n * 100), 3) if n else 0.0\n",
    "        level = \"high\" if pct > 15 else (\"medium\" if pct > 5 else \"low\")\n",
    "        rows.append((c, nulls, pct, level))\n",
    "\n",
    "\n",
    "    return pl.DataFrame(rows, schema=[\"column\", \"total_missing\", \"percent_missing\", \"missing_level\"]).sort(\"percent_missing\", descending=True)\n",
    "\n",
    "\n",
    "miss_df = missing_report(lf_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or this method using melt instead of python loops:\n",
    "\n",
    "def missing_report_fast(lf: pl.LazyFrame) -> pl.DataFrame:\n",
    "    stats = (\n",
    "        lf.select(\n",
    "            pl.len().alias(\"n_rows\"),\n",
    "            pl.all().null_count()                 # one scalar per column\n",
    "        )\n",
    "        .collect()                                # 1-row wide table\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        stats\n",
    "        .melt(id_vars=\"n_rows\",\n",
    "              variable_name=\"column\",\n",
    "              value_name=\"total_missing\")\n",
    "        .with_columns([\n",
    "            (pl.col(\"total_missing\") / pl.col(\"n_rows\") * 100)\n",
    "                .round(3)\n",
    "                .alias(\"percent_missing\"),\n",
    "            pl.when(pl.col(\"percent_missing\") > 15)\n",
    "              .then(\"high\")\n",
    "              .when(pl.col(\"percent_missing\") > 5)\n",
    "              .then(\"medium\")\n",
    "              .otherwise(\"low\")\n",
    "              .alias(\"missing_level\"),\n",
    "        ])\n",
    "        .select([\"column\", \"total_missing\", \"percent_missing\", \"missing_level\"])\n",
    "        .sort(\"percent_missing\", descending=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a516e-402f-4d80-870e-ba1d7eb5f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value_counts for columns with missing data, eager example\n",
    "collected = lf_dummies.collect()\n",
    "for c in collected.columns:\n",
    "    if collected[c].null_count() > 0:\n",
    "        print(\"\\n\", c)\n",
    "        print(collected[c].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f34d59-04aa-403d-bddc-c3b4b5a3ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with mean or other value\n",
    "lf_imputed = lf_dummies.with_columns(\n",
    "    pl.col(\"col\").fill_null(pl.col(\"col\").mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9f459-0bdc-48d3-a68e-d0b9567a88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with fewer than N non-nulls (here: at least 5 values)\n",
    "lf_threshold = (\n",
    "    lf_imputed.lazy()\n",
    "    .select([\n",
    "        *[\n",
    "            pl.when((pl.len() - pl.col(c).null_count()) >= 5)\n",
    "            .then(pl.col(c))\n",
    "            .otherwise(pl.lit(None).cast(pl.Null))\n",
    "            .alias(c)\n",
    "            for c in lf_imputed.collect_schema().names()\n",
    "        ]\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99414fdf-cc04-4cf2-a133-2f789de04203",
   "metadata": {},
   "source": [
    "### Duplicate checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc247-706c-4774-b81b-d501c417c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_summary(lf: pl.LazyFrame, subset: list[str] | None = None) -> dict:\n",
    "    df = lf.collect()\n",
    "    if subset:\n",
    "        partial = df.is_duplicated(subset=subset).sum()\n",
    "    else:\n",
    "        partial = 0\n",
    "    exact = df.is_duplicated().sum()\n",
    "    return {\"exact_duplicates\": int(exact), \"partial_duplicates\": int(partial)}\n",
    "\n",
    "\n",
    "_ = duplicate_summary(lf_imputed, subset=[\"col_a\", \"col_b\"])\n",
    "\n",
    "\n",
    "# Drop duplicate rows (all columns)\n",
    "lf_nodup = lf_imputed.unique(keep=\"first\")\n",
    "\n",
    "\n",
    "# Drop duplicates on a subset\n",
    "lf_nodup_subset = lf_imputed.unique(subset=[\"col_a\", \"col_b\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc22a09-cb8d-4acc-b6ef-69b2b54012da",
   "metadata": {},
   "source": [
    "### Outliers and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db675ba6-263c-44cb-b290-cc4b77d07dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier labeling with IQR (keeps rows, adds a boolean flag)\n",
    "\n",
    "def with_iqr_flag(lf: pl.LazyFrame, col: str, flag_name: str | None = None) -> pl.LazyFrame:\n",
    "    flag = flag_name or f\"{col}_is_outlier\"\n",
    "    q1 = pl.col(col).quantile(0.25, interpolation=\"nearest\")\n",
    "    q3 = pl.col(col).quantile(0.75, interpolation=\"nearest\")\n",
    "    iqr = (q3 - q1)\n",
    "    lo = q1 - 1.5 * iqr\n",
    "    hi = q3 + 1.5 * iqr\n",
    "    return lf.with_columns((~pl.col(col).is_between(lo, hi, closed=\"both\")).alias(flag))\n",
    "\n",
    "\n",
    "lf_flagged = with_iqr_flag(lf_nodup, \"num_col\")\n",
    "\n",
    "\n",
    "# If you need to actually filter outliers:\n",
    "lf_no_outliers = lf_flagged.filter(pl.col(\"num_col_is_outlier\").not_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c908c3-a22d-4014-a2b5-fec81435176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars doesnt have built-in plotting, so convert to pandas for visualization:\n",
    "df = lf_flagged.select([\"feature_a\", \"feature_b\"]).collect().to_pandas()\n",
    "\n",
    "# See pandas_qc_tools.ipynb for plotting examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f71442-a2f2-47a2-bde6-1a453f00ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform (create a new column, safe for negatives via clip)\n",
    "lf_log = lf_flagged.with_columns(\n",
    "    pl.col(\"num_col\").clip_min(0).add(1.0).log().alias(\"num_col_log1p\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec261f-7946-49c7-b392-de8ac59d218c",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a150e9-c5f2-491d-a669-33d3354a3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalize strings\n",
    "lf_text = (\n",
    "    lf_log.with_columns(\n",
    "        pl.col(\"column_str\")\n",
    "        .cast(pl.Utf8, strict=False)\n",
    "        .str.normalize(\"NFKC\")\n",
    "        .str.strip()\n",
    "        .str.to_lowercase()\n",
    "        .str.replace_all(\"&\", \"and\")\n",
    "        .str.replace_all(r\"[^\\w\\s]\", \"\")\n",
    "        .alias(\"column_str\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5baa1-2067-46ac-ba04-04c176a4a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize strings based on keyword. Example with colors\n",
    "\n",
    "category_map = {\n",
    "    \"blue\": [\"azure\", \"cerulean\", \"sky blue\"],\n",
    "    \"red\": [\"magenta\", \"dark red\", \"red orange\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Build an expression assigning the first matching label; else 'other'\n",
    "expr = pl.lit(\"other\")\n",
    "for label, words in category_map.items():\n",
    "    pat = \"|\".join(re.escape(w) for w in words)\n",
    "    expr = pl.when(pl.col(\"color\").cast(pl.Utf8, strict=False).str.to_lowercase().str.contains(pat)).then(pl.lit(label)).otherwise(expr)\n",
    "\n",
    "\n",
    "lf_text_cats = lf_text.with_columns(expr.alias(\"color_category\"))\n",
    "\n",
    "\n",
    "# Replace text value with contained keyword precedence\n",
    "lf_place_norm = (\n",
    "    lf_text_cats.with_columns(\n",
    "        pl.when(pl.col(\"Place\").cast(pl.Utf8).str.contains(\"name_a\", literal=True))\n",
    "        .then(pl.lit(\"name_a\"))\n",
    "        .when(pl.col(\"Place\").cast(pl.Utf8).str.contains(\"name_b\", literal=True))\n",
    "        .then(pl.lit(\"name_b\"))\n",
    "        .otherwise(pl.col(\"Place\").cast(pl.Utf8).str.replace_all(\"-\", \"_\"))\n",
    "        .alias(\"Place\")\n",
    "    )\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b156e3-05d3-4308-b393-e5ecd60a6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace text value with contained keyword\n",
    "place = df['Place']\n",
    "name_a = place.str.contains('name_a')\n",
    "name_b = place.str.contains('name_b')\n",
    "df['Place'] = np.where(name_a, 'name_a',\n",
    "                       np.where(name_b, 'name_b',\n",
    "                                place.str.replace('-', '_')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5286d-2a5e-49b6-b9ad-778fee990c3c",
   "metadata": {},
   "source": [
    "### Chain cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914b75b-f1c1-4e0e-9632-5d5b8aeb0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_lazy(\n",
    "    lf: pl.LazyFrame,\n",
    "    *,\n",
    "    dedup_subset: list[str] | None = None,\n",
    "    category_map: dict[str, list[str]] | None = None,\n",
    "    text_col: str | None = None,\n",
    "    new_col_name: str = \"category\",\n",
    ") -> tuple[pl.LazyFrame, pl.DataFrame]:\n",
    "    out = lf\n",
    "    if dedup_subset:\n",
    "        out = out.unique(subset=dedup_subset, keep=\"first\")\n",
    "\n",
    "\n",
    "    if category_map and text_col:\n",
    "        expr = pl.lit(\"other\")\n",
    "        for label, words in category_map.items():\n",
    "            pat = \"|\".join(re.escape(w) for w in words)\n",
    "            expr = (\n",
    "            pl.when(pl.col(text_col).cast(pl.Utf8, strict=False).str.to_lowercase().str.contains(pat))\n",
    "            .then(pl.lit(label))\n",
    "            .otherwise(expr)\n",
    "            )\n",
    "        out = out.with_columns(expr.alias(new_col_name))\n",
    "\n",
    "\n",
    "        # Missing report \n",
    "        mr = missing_report(out)\n",
    "        return out, mr\n",
    "\n",
    "\n",
    "cleaned_lf, missing_rep = clean_data_lazy(\n",
    "    lf_place_norm,\n",
    "    dedup_subset=[\"col_a\", \"col_b\"],\n",
    "    category_map=category_map,\n",
    "    text_col=\"color\",\n",
    "    new_col_name=\"color_category\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Collect final result (streaming recommended for large data)\n",
    "cleaned_df = cleaned_lf.collect(streaming=True)\n",
    "cleaned_df.write_csv(\"final_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
