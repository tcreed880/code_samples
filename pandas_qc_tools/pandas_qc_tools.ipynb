{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aab0715-b5a6-47ef-8e60-1164a5e6028f",
   "metadata": {},
   "source": [
    "## Pandas QC Tools\n",
    "\n",
    "This notebook is a reference collection of functions and tips for cleaning datasets using pandas. This is not designed to be run on a specific dataset, just provide snippets for common cleaning tasks.\n",
    "\n",
    "This notebook serves as a reference collection of functions and tips for cleaning datasets using pandas. Itâ€™s not designed to run end-to-end on a specific dataset, but rather to provide reusable snippets for common data cleaning tasks.\n",
    "\n",
    "### Set Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8531dd-ae1a-4b49-b29d-589b9a22fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check environment\n",
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0997b7-0d8b-493e-9361-e3314f965a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d649b9-0635-4529-9d0f-840769563311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('my_dataset.csv')\n",
    "\n",
    "# copy unaltered original dataset for reference\n",
    "raw_data = df.copy()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb5a5-6d8f-4535-8958-556ea2b1b69b",
   "metadata": {},
   "source": [
    "### Process large data sets in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360c862-1203-4205-891e-41ea99616ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set size of chunks based on available memory\n",
    "chunk_size = 10000 \n",
    "chunks = pd.read_csv('my_large_dataset.csv', chunksize=chunk_size)\n",
    "\n",
    "# Example of running a cleaning function on multiple chunks\n",
    "results = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # apply some cleaning function to column/series\n",
    "    chunk['cleaned_column'] = chunk['raw_column'].apply(column_cleaner_function)\n",
    "    \n",
    "    # optionally filter or aggregate cleaned series\n",
    "    filtered = chunk[chunk['cleaned_column'] == 'filter_value']\n",
    "    results.append(filtered)\n",
    "\n",
    "# Combine all cleaned chunks\n",
    "cleaned_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f623c239-ae17-46d6-abef-5a191e52bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write to disk instead of storing all chunks in memory\n",
    "chunk_size = 10000 \n",
    "chunks = pd.read_csv('my_large_dataset.csv', chunksize=chunk_size)\n",
    "\n",
    "output_file = 'cleaned_output.csv'\n",
    "first_chunk = True \n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # apply some cleaning function to column/series\n",
    "    chunk['cleaned_column'] = chunk['raw_column'].apply(column_cleaner_function)\n",
    "    \n",
    "    # optionally filter or aggregate cleaned series\n",
    "    filtered = chunk[chunk['cleaned_column'] == 'example_value']\n",
    "    \n",
    "    # write to the disk, only write header for the first chunk\n",
    "    filtered.to_csv(output_file, mode='a', index=False, header=first_chunk)\n",
    "    first_chunk = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e94f7e-2d20-4443-9131-5cf1f69fca36",
   "metadata": {},
   "source": [
    "### Clean up columns and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f8de0-6c08-46bf-9a8f-77ae5d2d0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# janitor module's method for column name cleaning\n",
    "df.clean_names()\n",
    "\n",
    "# clean column names with regular expressions\n",
    "df.columns = [\n",
    "    re.sub(r'\\s+', '_', re.sub(r'[^\\w\\s]', '', col)).strip().lower()\n",
    "    for col in df.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d4ed3-4550-424b-85db-87fa4399405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting columns\n",
    "to_drop = ['column_a', 'column_b']\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd0e5b-25a3-4e52-a948-56595ce1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set indexing column\n",
    "df['identifier'].is_unique\n",
    "df.set_index('identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22b002-abeb-40b5-b5ca-b0bd94a2cdca",
   "metadata": {},
   "source": [
    "### Fix datatypes and categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9154dfa-cc04-4389-a085-ce72f2d76f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types\n",
    "df['num_feature'] = df['num_feature'].astype(int)\n",
    "df['num_feature'] = df['num_feature'].astype('float16')\n",
    "df['str_feature'] = df['str_feature'].astype(str)\n",
    "\n",
    "df['date_col'] = pd.to_datetime(df['date_col'], errors='coerce')\n",
    "\n",
    "# Clip and cap values in a feature\n",
    "df['col'] = df['col'].clip(lower=0, upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d9190-11d4-4da0-8981-95e82d2ca05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each unique category into a unique integer\n",
    "df['cat_encoded'] = df['cat_uncoded'].astype('category').cat.codes\n",
    "\n",
    "# Create a new column for each category, with 1 if the row belongs to that category, else 0\n",
    "pd.get_dummies(df, columns=['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b350080-0925-4f4b-8227-8d252ecc5623",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bde80-a2f8-4533-b1a3-103d162b7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values with a heatmap\n",
    "sns.heatmap(df.isnull(), cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa260f-a8b4-4e95-a373-ca2365142d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return missing data report\n",
    "def check_missing_values(df):\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'total_missing': df.isnull().sum(),\n",
    "        'percent_missing': (df.isnull().sum() / len(df) * 100).round(3)\n",
    "    }).sort_values('percent_missing', ascending=False)\n",
    "    \n",
    "    missing_stats['missing_level'] = missing_stats['percent_missing'].apply(\n",
    "        lambda x: 'high' if x > 15 else ('medium' if x > 5 else 'low')\n",
    "    )\n",
    "    return missing_stats\n",
    "    \n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a516e-402f-4d80-870e-ba1d7eb5f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value_counts for columns with missing data, can be messy\n",
    "cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "\n",
    "for col in cols_with_na:\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f34d59-04aa-403d-bddc-c3b4b5a3ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with mean or other value\n",
    "df['col'] = df['col'].fillna(df['col'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9f459-0bdc-48d3-a68e-d0b9567a88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns with at least 5 non-NA values\n",
    "df = df.dropna(axis=1, thresh=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99414fdf-cc04-4cf2-a133-2f789de04203",
   "metadata": {},
   "source": [
    "### Duplicate checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc247-706c-4774-b81b-d501c417c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df, subset_columns=None):\n",
    "    duplicate_report = {\n",
    "        'exact_duplicates': df.duplicated().sum(),\n",
    "        'partial_duplicates': df.duplicated(subset=subset_columns).sum() if subset_columns else 0\n",
    "    }\n",
    "    return duplicate_report\n",
    "\n",
    "check_duplicates(df)\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# Remove duplicates based on subset of columns\n",
    "df.drop_duplicates(subset=['col_a', 'col_b'], inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc22a09-cb8d-4acc-b6ef-69b2b54012da",
   "metadata": {},
   "source": [
    "### Outliers and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40b13e-6cfa-4043-a60a-f7ea4d2853ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot for checking trends\n",
    "sns.scatterplot(x=df['feature_a'], y=df['feature2'])\n",
    "plt.title(\"Scatter Plot for Relationship Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db675ba6-263c-44cb-b290-cc4b77d07dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for all numerical features\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "n_cols = len(numeric_df.columns)\n",
    "\n",
    "fig, axes = plt.subplots(n_cols, 1)\n",
    "\n",
    "for i, col in enumerate(numeric_df.columns):\n",
    "    sns.boxplot(x=numeric_df[col], ax=axes[i])\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c908c3-a22d-4014-a2b5-fec81435176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for a numerical column\n",
    "df['num_col'].hist(bins=20)\n",
    "plt.title(\"Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f71442-a2f2-47a2-bde6-1a453f00ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation\n",
    "df['num_col'] = np.log1p(df['num_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52411f-f607-4c67-8505-9105fbd55510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outlier rows using IQR based rule\n",
    "Q1 = df['num_col'].quantile(0.25)\n",
    "Q3 = df['num_col'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out rows with outliers\n",
    "df = df[(df['num_col'] >= Q1 - 1.5 * IQR) & (df['num_col'] <= Q3 + 1.5 * IQR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec261f-7946-49c7-b392-de8ac59d218c",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a150e9-c5f2-491d-a669-33d3354a3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim spaces and convert to lowercase\n",
    "df['column_str'] = df['colum_str'].str.strip().str.lower()\n",
    "\n",
    "# Replace '&' with 'and'\n",
    "df['column_str'] = df['colum_str'].str.replace('&', 'and')\n",
    "\n",
    "# Remove punctuation\n",
    "df['column_str'] = df['colum_str'].str.replace(r'[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5baa1-2067-46ac-ba04-04c176a4a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize strings based on keyword. Example with colors\n",
    "\n",
    "category_map = {\n",
    "    'blue': ['AZURE', 'cerulean', 'sky blue'],\n",
    "    'red': ['magenta', 'DARK RED', 'red orange']\n",
    "}\n",
    "\n",
    "def categorize_column(series, category_map):\n",
    "    series = series.str.lower().fillna('') # clean up string formatting\n",
    "    conditions = []\n",
    "    choices = []\n",
    "\n",
    "    for label, keywords in category_map.items():\n",
    "        pattern = '|'.join(map(re.escape, keywords)) # use regex to escape special characters, create keyword search list\n",
    "        conditions.append(series.str.contains(pattern, regex=True)) # check for keyword matches\n",
    "        choices.append(label)\n",
    "\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Example usage\n",
    "categorize_column(df['color'], category_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b156e3-05d3-4308-b393-e5ecd60a6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace text value with contained keyword\n",
    "place = df['Place']\n",
    "name_a = place.str.contains('name_a')\n",
    "name_b = place.str.contains('name_b')\n",
    "df['Place'] = np.where(name_a, 'name_a',\n",
    "                       np.where(name_b, 'name_b',\n",
    "                                place.str.replace('-', '_')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5286d-2a5e-49b6-b9ad-778fee990c3c",
   "metadata": {},
   "source": [
    "### Chain cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914b75b-f1c1-4e0e-9632-5d5b8aeb0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of combining multiple cleaning functions together\n",
    "\n",
    "def clean_data(df, category_map, text_col, new_col_name):\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates(subset=['col_a', 'col_b'])\n",
    "\n",
    "    # Categorize column \n",
    "    df[new_col_name] = categorize_column(df[text_col], category_map)\n",
    "\n",
    "    # Report missing values\n",
    "    missing_report = check_missing_values(df)\n",
    "\n",
    "    return df, missing_report\n",
    "\n",
    "cleaned_df, missing_report = clean_data(df, category_map, 'color', 'color_category')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
